### Funnel 분석 
#### 전환율이 가장 높았던 무료 채널을 알아보자 
생각 정리 
```sql
[상황]
Funnel 분석을 진행하다가,
무료 채널로 유입 된/유료 채널로 유입 된 유저들의 특성이 다를 수 있겠다는 생각이 들었다. 

초기 스타트업이였던 데이터리안을 무료 채널로 유입이 되려면
적극적오르 데이터 분석과 관련된 정보를 찾으려고 했던 사람이 
구글링을 통해서 데이터리안 brunch, velog 혹은 관련 세미나나 아티클을 통해서 
데이터리안을 알게되어 유입되었다 로 가정할 수 있기 때문이다.

최근 콘텐츠를 통한 마케팅이 대두되고 있기 때문에
아! 무료 채널의 어떤 콘텐츠에서 pv가 많이 발생했을까? 
이걸 분석해서 추후에 높은 pv를 발생시킬 수 있는 콘텐츠를 발행하는데 인사이트를 줄 수 있지 않을까? 라는 생각에 

무료 채널의 어떤 content에서 pv가 많이 발생했는지 알아보려고 했다.

분석을 진행하다 보면 자꾸 구렁에 빠지는 느낌이 드는데, 이번에도 그랬던거 같다.
약간 주객전도가 된 느낌... 하지만 우여곡절을 기록해보기로 했다.

[초기]
utm_content를 기준으로 분석을 진행하면 되겠다 라는 생각이였다.
하지만, campaign만 존재하는 경우도 있어 campaign과 content를 함께 고려하기로 했다. 

page_location에 campaign과 content 정보가 함께 들어가 있는데 (ex. page_location = utm_campaign=~~~&utm_content=~~~) 이걸 분리시키기 위해서 파이썬을 이용했다

그 다음 MySQL에 해당 csv 파일을 연동시켜 분석하였다. 
```

```python
import pandas as pd 
import re 

# 5000row 이상의 데이터는 불러오지 못해 source별 개별 추출하였다
x = pd.read_csv("c:/data/direct_youtube.csv")
y = pd.read_csv("c:/data/etc.csv")
a = pd.read_csv("c:/data/fb_linktree_brunch.csv")

# 개별 추출된 데이터 합치기 
ga = pd.concat([x,y,a],ignore_index=True)
ga.info()

# 사본으로 저장하기 
ga_new = ga.copy()

# 전처리를 위해 타입 변경 
ga_new['page_location']  = ga_new['page_location'].astype('str')
ga_new['page_location'].dtypes # 확인 

# campaign, content 컬럼 생성  
ga_new['campaign']=ga_new['page_location'].str.findall("(utm_campaign=\w+&?)")
ga_new['content']=ga_new['page_location'].str.findall("(utm_content=\w+&?)")

ga_new[['campaign','content']] # 확인 

# campaign 전처리 
ga_new['campaign'] =ga_new['campaign'].astype('str')
ga_new['campaign'] = ga_new['campaign'].replace("utm_campaign=","",regex=True)
ga_new['campaign'] = ga_new['campaign'].replace("&","",regex=True)
ga_new['campaign'] = ga_new['campaign'].replace("\]","",regex=True) # |을 통해서 함께 전처리 하고싶었는데 왜 안될까? 
ga_new['campaign'] = ga_new['campaign'].replace("\[","",regex=True)


# content 전처리 
ga_new['content'] =ga_new['content'].astype('str')
ga_new['content'] = ga_new['content'].replace("utm_content=","",regex=True)
ga_new['content'] = ga_new['content'].replace("&","",regex=True)
ga_new['content'] = ga_new['content'].replace("\[","",regex=True)
ga_new['content'] = ga_new['content'].replace("\]","",regex=True)


# MySQL 연동하기 위해 csv 파일 저장하기 
ga_new.to_csv("c:/data/ga_ansi.csv",encoding='ANSI') # utf-8 하니까 MySQL 내에서 에러 발생
```

```sql
with temp as(
select source,
		campaign,
		content,
		count(distinct user_pseudo_id, ga_session_id) as cnt 
from ga_ansi
where campaign is not null
and content is not null 
and event_name = 'page_view'
group by 1,2,3
order by 4 desc )
, temp2 as (
select *,
		sum(cnt) over(partition by source) as amount_cnt -- source별 전체 pv값 옆에 붙이기 
from temp)

select source,
				campaign,
				content,
				cnt,
				round((cnt/amount_cnt),2)*100 as per -- 해당 source 내에서 pv 지분을 몇 % 차지하는지
from temp2
```

#### 결과 분석
```sql
무료 채널 중 utm_campaign, utm_content가 발생되는 brunch, careerly, publy, youtube를 기준 pv top 5

velog에서 발생한 pv는 모두 sql_camp advanced_feb에서 발생함
brunch에서 발생한 pv는 sql_camp advanced_feb → sql_book_recommendation → xxit_qna의 순으로 발생함 
publy에서 발생한 pv는 모두 kakaotalk_gift에서 발생함 

content가 가장 많이 발행된 brunch를 기준으로 brunch 내에서 어떤 content가 pv를 가장 많이 발생시켰는지 확인함

sql_camp에 대한 소개글 content가 pv를 총 20번 발생시켰고,
그 다음은 xxit_qna와 관련된 자격증이 꼭 필요한가요? DA 직무에 대한 소개글(jobs,skill)에 관련된 content에서 pv가 총 20번 발생 
마지막이 sql 책 추천 소개글에서 pv를 총 14번 발생시킴

종합해서 보았을 때,

- SQL CAMP에 대한 혜택(advanced_feb라고 해서 ^^;)과 관련된 content에서 pv가 가장 많이 발생되었다
    - SQL CAMP와 관련된 이벤트(ex.할인 등) 진행 시 무료 채널에 관련 content를 발행해보자
- 타켓 고객의 궁금증을 해소하는 content에서 pv가 많이 발생되었다.
    - SQL_CAMP를 진행하며 참여자들에게 인터뷰를 진행하여 핵심 타겟 고객의 궁금증을 해소할 수 있는 관련 content를 발행해보자

```

